{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEBench Agent Local Evaluation\n",
    "\n",
    "This notebook runs the SWEBench agent evaluation locally without using Modal. It's based on the code in the `swebench_agent_run` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import traceback\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import SWEBench utilities\n",
    "from codegen.extensions.swebench.utils import SWEBenchDataset, SweBenchExample, get_swe_bench_example, get_swe_bench_examples\n",
    "from codegen.extensions.swebench.harness import run_agent_on_entry\n",
    "from codegen.extensions.swebench.report import generate_report\n",
    "from codegen.sdk.core.codebase import Codebase\n",
    "\n",
    "# Set up directories for predictions and logs\n",
    "PREDS_DNAME = Path(\"predictions\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PREDS_DNAME.mkdir(exist_ok=True, parents=True)\n",
    "LOG_DIR.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Local Processing Functions\n",
    "\n",
    "Now, let's define functions to process SWEBench examples locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def process_example(example: SweBenchExample):\n",
    "    \"\"\"Process a single SWEBench example locally.\"\"\"\n",
    "    try:\n",
    "        # Create a Codebase object for the example\n",
    "        codebase = Codebase.from_repo(repo_full_name=example.repo, commit=example.base_commit, language=\"python\")\n",
    "        \n",
    "        # Run the agent on the example\n",
    "        result = await run_agent_on_entry(example, codebase=codebase)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        error_info = {\n",
    "            \"error_type\": error_type,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exception(type(e), e, e.__traceback__),\n",
    "        }\n",
    "        \n",
    "        print(f\"Error processing {example.instance_id}:\")\n",
    "        print(f\"Type: {error_type}\")\n",
    "        print(f\"Message: {str(e)}\")\n",
    "        print(\"Traceback:\")\n",
    "        print(\"\".join(error_info[\"traceback\"]))\n",
    "        \n",
    "        return {\"instance_id\": example.instance_id, \"status\": \"error\", \"error_info\": error_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def process_batch_locally(examples: list[SweBenchExample], batch_size=3):\n",
    "    \"\"\"Process a batch of examples concurrently but locally.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of SweBenchExample objects to process\n",
    "        batch_size: Number of examples to process concurrently.\n",
    "                   Default is 3 which is reasonable for local processing.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Process examples in batches\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i : i + batch_size]\n",
    "\n",
    "        # Create tasks for this batch\n",
    "        batch_tasks = [process_example(example) for example in batch]\n",
    "\n",
    "        # Wait for all tasks in this batch to complete\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{len(examples) // batch_size + 1} (examples {i + 1}-{min(i + batch_size, len(examples))})\")\n",
    "\n",
    "        try:\n",
    "            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)\n",
    "\n",
    "            # Store results\n",
    "            for example, result in zip(batch, batch_results):\n",
    "                if isinstance(result, Exception):\n",
    "                    error_type = type(result).__name__\n",
    "                    error_info = {\n",
    "                        \"error_type\": error_type,\n",
    "                        \"error_message\": str(result),\n",
    "                        \"traceback\": traceback.format_exception(type(result), result, result.__traceback__),\n",
    "                    }\n",
    "\n",
    "                    print(f\"Error processing {example.instance_id}:\")\n",
    "                    print(f\"Type: {error_type}\")\n",
    "                    print(f\"Message: {str(result)}\")\n",
    "                    print(\"Traceback:\")\n",
    "                    print(\"\".join(error_info[\"traceback\"]))\n",
    "\n",
    "                    results.append({\"instance_id\": example.instance_id, \"status\": \"error\", \"error_info\": error_info})\n",
    "                else:\n",
    "                    if result is None:\n",
    "                        print(f\"Warning: Null result for {example.instance_id}\")\n",
    "                        results.append({\"instance_id\": example.instance_id, \"status\": \"error\", \"error_info\": {\"error_type\": \"NullResult\", \"error_message\": \"Process returned None\"}})\n",
    "                    else:\n",
    "                        results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Batch processing error:\")\n",
    "            print(f\"Type: {type(e).__name__}\")\n",
    "            print(f\"Message: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "            # Mark all examples in the batch as failed\n",
    "            for example in batch:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"instance_id\": example.instance_id,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error_info\": {\"error_type\": type(e).__name__, \"error_message\": str(e), \"traceback\": traceback.format_exc(), \"batch_failure\": True},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def run_local_eval(dataset_name: str = SWEBenchDataset.LITE.value, length: int = 3, instance_id: str = None):\n",
    "    \"\"\"Run the evaluation locally.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: The name of the dataset to use.\n",
    "        length: The number of examples to process.\n",
    "        instance_id: The instance ID of a specific example to process.\n",
    "    \"\"\"\n",
    "    run_id = str(uuid.uuid4())\n",
    "    predictions_dir = PREDS_DNAME / f\"results_{run_id}\"\n",
    "    dataset = SWEBenchDataset(dataset_name)\n",
    "    \n",
    "    if instance_id:\n",
    "        examples = [get_swe_bench_example(instance_id, dataset=dataset)]\n",
    "    else:\n",
    "        examples = get_swe_bench_examples(dataset=dataset, length=length)\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {len(examples)} examples...\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Create a timestamp for this run\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Process all examples in parallel batches\n",
    "        results = await process_batch_locally(examples)\n",
    "\n",
    "        # Save individual results\n",
    "        for result in results:\n",
    "            if result and \"instance_id\" in result:\n",
    "                instance_id = result[\"instance_id\"]\n",
    "                output_file = predictions_dir / f\"{instance_id}.json\"\n",
    "                output_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    json.dump(result, f, indent=4)\n",
    "\n",
    "        # Save summary file\n",
    "        summary_file = predictions_dir / f\"summary_{timestamp}.json\"\n",
    "        summary = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"total_examples\": len(examples),\n",
    "            \"successful\": len([r for r in results if r and \"status\" not in r]),\n",
    "            \"failed\": len([r for r in results if r and \"status\" in r and r[\"status\"] == \"error\"]),\n",
    "            \"error_types\": {},\n",
    "            \"results\": results,\n",
    "        }\n",
    "\n",
    "        # Collect error statistics\n",
    "        for result in results:\n",
    "            if result and \"status\" in result and result[\"status\"] == \"error\":\n",
    "                error_type = result.get(\"error_info\", {}).get(\"error_type\", \"Unknown\")\n",
    "                summary[\"error_types\"][error_type] = summary[\"error_types\"].get(error_type, 0) + 1\n",
    "\n",
    "        with open(summary_file, \"w\") as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "\n",
    "        print(\"\\nProcessing complete!\")\n",
    "        print(f\"Results saved to: {predictions_dir}\")\n",
    "        print(f\"Summary saved to: {summary_file}\")\n",
    "        print(f\"Successful: {summary['successful']}/{summary['total_examples']}\")\n",
    "        print(f\"Failed: {summary['failed']}/{summary['total_examples']}\")\n",
    "        if summary[\"error_types\"]:\n",
    "            print(\"\\nError type distribution:\")\n",
    "            for error_type, count in summary[\"error_types\"].items():\n",
    "                print(f\"  {error_type}: {count}\")\n",
    "                \n",
    "        # Generate report locally\n",
    "        try:\n",
    "            generate_report(predictions_dir, LOG_DIR, dataset, run_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating report: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        return summary, predictions_dir\n",
    "    except Exception as e:\n",
    "        print(\"Fatal error in run_local_eval:\")\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Single Example\n",
    "\n",
    "Let's run a single example to test our local evaluation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get a few examples from the dataset\n",
    "examples = get_swe_bench_examples(dataset=SWEBenchDataset.LITE, length=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available instance IDs:\")\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"{i + 1}. {example.instance_id} - {example.repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first example\n",
    "selected_example = examples[10]\n",
    "print(f\"\\nSelected example: {selected_example.instance_id} - {selected_example.repo}\")\n",
    "\n",
    "\n",
    "# Process the selected example directly\n",
    "async def process_single_example():\n",
    "    # Create a Codebase object for the example\n",
    "    try:\n",
    "        codebase = Codebase.from_repo(repo_full_name=selected_example.repo, commit=selected_example.base_commit, language=\"python\")\n",
    "\n",
    "        # Run the agent on the example\n",
    "        result = run_agent_on_entry(selected_example, codebase=codebase)\n",
    "\n",
    "        # Save the result\n",
    "        run_id = str(uuid.uuid4())\n",
    "        predictions_dir = PREDS_DNAME / f\"results_{run_id}\"\n",
    "        predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        output_file = predictions_dir / f\"{selected_example.instance_id}.json\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "        print(\"\\nProcessing complete!\")\n",
    "        print(f\"Result saved to: {output_file}\")\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run the processing\n",
    "result = await process_single_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
